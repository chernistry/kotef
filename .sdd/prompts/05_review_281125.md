# Kotef Architecture Review — 28.11.2025

## Deep Context Reference

For detailed architecture diagrams, complete agent flow (Mermaid), state structure, edge cases, and known issues, see:

**`/Users/sasha/IdeaProjects/personal_projects/kotef/.sdd/prompts/06_research_281125.md`**

This document contains:
- Full Mermaid flowcharts for main graph, each node, and SDD orchestrator
- Complete `AgentState` TypeScript interface
- Edge case handling tables
- Prompt roles and responsibilities
- Known architectural problems

---

## Context

**Kotef** is an autonomous AI coding agent built on Node.js/TypeScript/LangGraph.js. It uses SDD (Spec-Driven Development) as its "brain" for planning and executing tasks. Integrates with external LLMs (OpenRouter) for planning/research and Kiro CLI for coding.

**Current state**: MVP works, but the implementation feels over-engineered in some places while missing important aspects in others. Project is maintained by a single developer.

---

## Architectural Questions for Analysis

### 1. Repo Discovery — Is it deep enough?

**What exists:**
- `project_summary.ts` — basic analysis: languages by extensions, frameworks from package.json
- `code_index.ts` — ts-morph index for TS/JS (symbols, functions)
- `code_index_treesitter.ts` — tree-sitter for Python/Go/Rust/Java
- `contextScan` — cwd, files, gitStatus, readmeSummary

**What's missing (compared to modern agents):**
- Semantic code search (embeddings)
- Import graph / dependency tree analysis
- Understanding of architectural patterns (MVC, Clean Architecture)
- LSP integration for type information
- "Hot paths" in code

**Question:** Should we add semantic search or is improving tree-sitter + import graph sufficient?

---

### 2. Context Loss at OpenRouter → Kiro Handoff

**Flow:**
```
OpenRouter (planner/researcher) → state.sdd.ticket → kiro_coder.ts → Kiro CLI
```

**What's lost when passing to Kiro:**
- `state.researchResults` — web research findings
- `state.impactMap` — affected files analysis
- `state.riskMap` — risk assessment
- `state.clarified_goal` — clarified goal with constraints
- `state.shapedGoal` — appetite, non-goals
- `state.projectSummary` — stack understanding
- `state.diagnosticsLog` — error history

**Kiro only receives:**
- Ticket content
- Brief architect summary (20 lines)
- Brief best_practices (15 lines)

**Question:** Should we pass full context? Or would that bloat the prompt?

---

### 3. Context Drift — Specific Points

- `state.sdd.ticketId` was not passed to state (fixed)
- Goal constraints ("DO NOT REDESIGN") are not parsed and not passed to ticket generation
- `loopCounters` reset between runs, no persistence
- `researchResults` from SDD orchestrator are not passed to main agent state

---

### 4. Repo Discovery as a Separate Tool?

**Current:** `buildProjectSummary()` is called once in planner node.

**Proposal:** Tool `repo_discovery` with parameters:
- `mode: 'quick' | 'deep' | 'semantic'`
- `focus: 'architecture' | 'dependencies' | 'patterns'`

**Benefits:** Planner can call repeatedly, incremental discovery, caching.

---

### 5. Integration with Other Agents

**Options:**
- **gemini-cli**: Google's Gemini, good tool calling, but vendor lock-in
- **qwen-code**: Open source, good coding capabilities, requires own inference
- **qwen-agent**: Full agent framework, but Python-based

**Question:** Fork or integrate as external coder (like Kiro)?

---

### 6. Kotef as MCP Server / "Exoskeleton"?

**Alternative architecture:**
```
Any AI IDE (Cursor/Windsurf/Kiro) → kotef MCP server
                                  ↓
                            SDD management
                            Research orchestration
                            Ticket lifecycle
                            Verification pipeline
```

**Pros:** Any IDE can use kotef's "brain", no coding duplication.
**Cons:** MCP is not yet stable, harder debugging.

---

### 7. Instruction Following and Architecture Rigidity

**Problem:** Agent ignores explicit constraints in goal (e.g., "DO NOT REDESIGN COMPLETELY" → generates full redesign).

**What's hardcoded:**
- `planner.md` — fixed prompt with JSON schema, doesn't adapt to goal
- `sdd_orchestrator.ts` — linear flow without intent/constraint analysis
- Ticket generation — template approach, doesn't consider "appetite" and "non-goals"
- `taskScope` determined by heuristics, not LLM analysis
- Execution profiles are predefined, not derived from context

**What's lost:**
- User constraints ("DO NOT...", "only minor...")
- Implicit intent (polish vs redesign vs refactor)
- Scope boundaries (which files NOT to touch)

**Questions:**
- Do we need an "intent parser" node before the flow?
- Make flow more dynamic (LLM decides next step)?
- "Constraint validator" before ticket generation?
- Are we relying too much on structured output instead of natural reasoning?

---

### 8. Prompt Fragmentation — Too Many Small Steps?

**Current LLM call chain:**
```
goal → research prompt → findings
     → best_practices prompt → best_practices.md
     → architect prompt → architect.md  
     → ticket_plan prompt → ticket list
     → ticket_generate prompt × N → N ticket files
     → planner prompt → decision
     → researcher prompt → more findings
     → coder prompt → code
     → verifier prompt → validation
```

**Problem:**
- Each prompt sees only its slice of context
- No "big picture" reasoning across the entire flow
- Constraints from goal dilute through the chain

**Alternative — consolidated prompts:**
| Instead of | Merge into |
|------------|------------|
| research + best_practices + architect | "Understand & Design" |
| ticket_plan + ticket_generate × N | "Plan Work" (all tickets at once) |
| planner + researcher | "Think & Research" |
| verifier + retrospective | "Validate & Learn" |

**Question:** How many LLM calls are really needed? Maybe one "mega-prompt" with chain-of-thought?

---

### 9. Excessive Planner ↔ Verifier ↔ Coder Cycles

**Rigid graph:**
```
coder → verifier (ALWAYS)
verifier → planner (ALWAYS, even if everything is OK)
planner → decides where to go next
```

**Inefficiency:**
- Each cycle = at least 2 LLM calls
- Verifier runs commands even for trivial changes
- Planner re-evaluates after every change

**Question:** Is verifier needed for every change? "Fast path" for small changes?

---

### 10. Research Duplication Between SDD Orchestrator and Runtime

**Flow:**
```
SDD Orchestrator: deepResearch() → best_practices.md → architect.md → tickets
Runtime Agent: researcher node → deepResearch() AGAIN for the same goal
```

**Results from first research are not passed to runtime state!**

---

### 11. No Memory/Learning Between Runs

- Each run starts with clean state
- No history of successful/unsuccessful approaches
- No cache of research results
- `loopCounters` reset, `progressHistory` not persisted

**Question:** Do we need `.sdd/cache/agent_memory.json` for cross-run learning?

---

### 12. Synchronous Blocking Flow Without Parallelism

```typescript
// In deepResearch:
for (const q of queries) {
    const results = await webSearch(cfg, q, ...);  // Sequential!
}
```

**Question:** Can we parallelize independent operations?

---

### 13. No Early Termination / Confidence Threshold

- Agent continues until `done` or budget exhaustion
- No "good enough" threshold
- No early exit on high confidence
- Verifier always runs full command set

---

### 14. Suboptimal Context Passing to Prompts

```typescript
'{{SDD_PROJECT}}': summarize(state.sdd.project, 2500),
'{{SDD_ARCHITECT}}': summarize(state.sdd.architect, 2500),
```

**Problem:** Fixed truncation limits regardless of:
- Model's context window size
- Section relevance to the task
- What was already passed in previous steps

**Question:** Dynamic context selection based on task relevance?

---

### 15. Budget System Too Coarse

```typescript
const budgets = {
    'fast-normal': { maxCommands: 30, maxTestRuns: 5, maxWebRequests: 15 },
};
```

- Fixed numbers don't account for task complexity
- No adaptation based on progress
- Budget exhaustion = hard stop, no graceful degradation

**Question:** Budget as advisory rather than hard limit?

---

### 16. Ticket Generation Ignores Existing Code Structure

In `sdd_orchestrator.ts` not passed:
- `projectSummary` (which files/modules exist)
- `codeIndex` (which functions/classes already exist)
- Existing patterns in code

**Result:** Tickets may propose creating what already exists.

---

## Summary of Key Problems

| # | Problem | Impact | Priority |
|---|---------|--------|----------|
| 7 | Constraint ignoring | Wrong tickets | HIGH |
| 8 | Prompt fragmentation | Context drift | HIGH |
| 2 | Context loss OpenRouter→Kiro | Incomplete info for coder | HIGH |
| 10 | Research duplication | Time + money | MEDIUM |
| 9 | Excessive cycles | Extra LLM calls | MEDIUM |
| 11 | No cross-run memory | Repeating mistakes | MEDIUM |
| 16 | Tickets without code awareness | Irrelevant plans | MEDIUM |
| 13 | No early termination | Overkill for simple tasks | LOW |
| 12 | No parallelism | Slow | LOW |
| 14 | Fixed truncation | Context loss | LOW |

---

## Expected Outcome

Based on this analysis and web research results (see `06_research_281125.md`):

1. Update `.sdd/architect.md` if architectural changes are needed
2. Create tickets in `.sdd/backlog/tickets/open/` to address problems
3. Prioritize considering the project is maintained by a single developer
4. Focus on simplification where possible, not adding complexity

---

## Decision-Making Methodology (MCDM)

When choosing between alternative solutions, use a weighted approach:

### Evaluation Criteria

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Impact** | 0.25 | How much the solution improves core functionality |
| **Effort** | 0.20 | Implementation complexity for solo developer (inverse: less effort = better) |
| **Risk** | 0.20 | Risk of breaking existing functionality (inverse) |
| **Simplification** | 0.15 | Does it simplify architecture or add complexity |
| **Maintainability** | 0.10 | Long-term maintainability |
| **Reversibility** | 0.10 | Can it be rolled back if it doesn't work |

### Rating Scale
- **9**: Excellent / Minimal effort/risk
- **7**: Good
- **5**: Average
- **3**: Poor
- **1**: Very poor / Maximum effort/risk

### Example Decision Matrix

| Alternative | Impact | Effort | Risk | Simplification | Maintainability | Reversibility | Score |
|-------------|--------|--------|------|----------------|-----------------|---------------|-------|
| A: Quick fix | 5 | 9 | 7 | 3 | 5 | 9 | ? |
| B: Refactor | 9 | 3 | 5 | 7 | 9 | 5 | ? |
| C: Rewrite | 9 | 1 | 3 | 5 | 7 | 1 | ? |

### Principles for Solo Developer

1. **Prefer incremental over big-bang**: Small changes with fast feedback loop
2. **Simplify before adding**: Remove unnecessary parts first, then add new ones
3. **80/20 rule**: Focus on 20% of changes that give 80% of results
4. **Reversibility matters**: Avoid irreversible architectural decisions
5. **Working > Perfect**: A working solution is better than a perfect plan

### Red Flags (avoid)

- Solutions requiring changes to >10 files simultaneously
- New abstractions without clear necessity
- "Future-proofing" for hypothetical scenarios
- Dependencies on unstable/experimental technologies
- Changes breaking existing tests without good reason

### Green Flags (prefer)

- Removing code instead of adding
- Consolidating duplicate logic
- Improving error messages and observability
- Adding tests for existing code
- Documenting non-obvious decisions
